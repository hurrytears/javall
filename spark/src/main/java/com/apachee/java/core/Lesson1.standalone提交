spark 执行流程（standalone）
Application（自己的spark 程序）copy 到本地机器
spark-submit(shell) 提交
本地客户端启动 Driver，相当于是一个进程：spark-submit使用我们之前一直使用的提交模式提交的时候，
    其实是会通反射的方式创建和构造一个driver actor出来
driver 会执行application 程序，也就是我们自己编写的代码：大家回想一下，我们的程序的第一行是什么，
    构造sparkconf ，然后构造sparkcontext
SparkContext 被构造出来，：在初始化的时候最重要的两件事就是构造出DAGscheduler 和TaskScheduler

taskScheduler: 实际上是会负责他对应的一个后台进程去连接master,向master注册application，这个很重要，开始连接集群
master:接受到application注册的请求之后，会使用资源调度算法，在spark集群的worker上为applicaion启动 多个 excutor
worker：会为application启动excutor(也是一个进程，sparkcontext只是一个对象)
excutor:启动之后会自己反向注册到TaskScheduler上去；
构造sparkcontext做完了

所有的excutor都反向注册到TaskScheduler上后，spakcontext会继续执行我们的代码：
每执行一个action，就会创建一个job
job 提交给DAGScheduler，会做一件非常非常关键的事，会将job划分为多个stage，然后每个stage创建一个TaskSet（stage划分算法）
    然后扔给TaskScheduler
TaskScheduler会把TaskSet中的每一个task提交到excutor上去执行： Task分配算吗
excutor内部有个线程池，每接受到一个task，都会用TaskRunner封装task,然后从线程池里取出一个线程，执行这个task
TaskRunner将我们编写的代码，也就是要执行的算子以及函数，拷贝，反序列化，然后执行Task
Task有两种，shufflemap task 和ReduceTask,只有最后一个stage是ReduceTask, 其它的都是shuffleMap Task

所以，spark应用的执行，就是stage分批次作为taskset提交到excutor执行，每个task针对RDD的一个partition，执行我们定义的算子和函数
以此类推，直到所有操作完成为止
